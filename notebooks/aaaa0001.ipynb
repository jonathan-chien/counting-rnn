{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from general_utils import fileio as fileio_utils\n",
    "import plotting\n",
    "\n",
    "import torch\n",
    "\n",
    "from engine import driver\n",
    "from general_utils import config as config_utils\n",
    "from general_utils import tensor as tensor_utils\n",
    "from analysis.utils import pca\n",
    "\n",
    "plt.close('all')\n",
    "gc.collect()\n",
    "\n",
    "# To get interactive plots, do the following: install ipympl package and\n",
    "# restart VS code. Then, run either %matplotlib widget or %matplotlib ipympl in \n",
    "# the notebook.\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3de244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad_and_concat_sequences(sequences, lengths):\n",
    "    \"\"\" \n",
    "    Takes in two tensors, `sequences` and `lengths`, of shape (N, L, D) and\n",
    "    (L,), respectively, where N = number of sequences, L = max sequence length, D =\n",
    "    dimension of sequence data. The sequences are then unpadded and\n",
    "    concatenated.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sequences : 3D tensor of shape (N, L, D).\n",
    "    lengths : 1D tensor of shape (L,)\n",
    "    \"\"\"\n",
    "    tensor_utils.validate_tensor(sequences, dim=3)\n",
    "    tensor_utils.validate_tensor(lengths, dim=1)\n",
    "    \n",
    "    list_of_sequences = torch.nn.utils.rnn.unpad_sequence(sequences, lengths, batch_first=True)\n",
    "    return torch.cat(list_of_sequences, dim=0)\n",
    "\n",
    "def unpad_and_concat_iterable_of_sequences(sequences_iter, lengths_iter):\n",
    "\n",
    "    list_of_sequences_concat = [\n",
    "        unpad_and_concat_sequences(sequences, lengths)\n",
    "        for (sequences, lengths) in zip(sequences_iter, lengths_iter)\n",
    "    ]\n",
    "    return torch.cat(list_of_sequences_concat)\n",
    "    \n",
    "def get_network_test_output(logs, seed_idx, test_run_id, output_dict='joined', output_key='hidden'):\n",
    "    \"\"\" \n",
    "    Based on returned items from engine.eval.process_batch_eval.\n",
    "    \"\"\"\n",
    "    return logs.iloc[seed_idx]['test_runs'][test_run_id]['logger_test'].get_entry(\n",
    "        level='batch', epoch_idx=0, batch_idx=0\n",
    "    )[output_dict][output_key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72194115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name/parameters.\n",
    "exp_date = '2025-09-10'\n",
    "exp_id = '0001'\n",
    "seed_idx_list = []\n",
    "test_run_id_list = []\n",
    "\n",
    "# Build path to experiment.\n",
    "notebooks_dir = Path(os.getcwd())\n",
    "top_level_dir = notebooks_dir.parent\n",
    "exp_dir = f'{top_level_dir}/experiments/{exp_date}/{exp_id}'\n",
    "\n",
    "# Retrieve all seed indices if none specified.\n",
    "if len(seed_idx_list) == 0:\n",
    "    seed_idx_list = [\n",
    "        int(p.name[-2:]) for p in (Path(exp_dir)).iterdir() if 'seed' in p.name\n",
    "    ]\n",
    "    seed_idx_list.sort()\n",
    "    print(f\"Seed idx list: {seed_idx_list}.\")\n",
    "\n",
    "# Retrieve logged items from experiment.\n",
    "logs = []\n",
    "for seed_idx in seed_idx_list:\n",
    "    # Build path to seed directory.\n",
    "    seed_dir = fileio_utils.get_dir(exp_dir) / f'seed{seed_idx:02d}'\n",
    "\n",
    "    # Set up dict to store all results for current seed.\n",
    "    seed_dict = {}\n",
    "    seed_dict['seed_idx'] = seed_idx\n",
    "    seed_dict['seed_dir'] = str(seed_dir)\n",
    "    seed_dict['test_runs'] = {}\n",
    "    \n",
    "    # Automatically get all test run IDs if none provided.\n",
    "    if len(test_run_id_list) == 0:\n",
    "        test_run_id_list = [\n",
    "            p.name for p in (seed_dir / 'test').iterdir() if p.is_dir()\n",
    "        ] # Check why this is not in order\n",
    "        test_run_id_list.sort()\n",
    "    \n",
    "    for test_run_id in test_run_id_list:\n",
    "        # Build path to test directory and record path. \n",
    "        test_dir = seed_dir / 'test' / test_run_id\n",
    "        \n",
    "        # Dict to contain all results for current test run. \n",
    "        seed_dict['test_runs'][test_run_id] = {}\n",
    "        seed_dict['test_runs'][test_run_id]['test_dir'] = str(test_dir)\n",
    "        seed_dict['test_runs'][test_run_id]['batch'] = pd.read_json(\n",
    "            test_dir / 'output' / 'logs' / 'test_batch_log.jsonl', lines=True\n",
    "        )\n",
    "        seed_dict['test_runs'][test_run_id]['epoch'] = pd.read_json(\n",
    "            test_dir / 'output' / 'logs' / 'test_epoch_log.jsonl', lines=True\n",
    "        )\n",
    "\n",
    "        # Prepare filepaths/IDs for running testing for trained model to get outputs.\n",
    "        configs_dir = test_dir / 'configs'\n",
    "        train_run_id = '_'.join(test_run_id.split('_', -1)[0:4])\n",
    "        model_filepath = driver.get_model_filepath(exp_dir, seed_idx, train_run_id, model_suffix='_best.pt')\n",
    "\n",
    "        # Run testing.\n",
    "        print(f\"Seed idx: {seed_idx}.\")\n",
    "        (\n",
    "            seed_dict['test_runs'][test_run_id]['logger_test'], \n",
    "            seed_dict['test_runs'][test_run_id]['model'], \n",
    "            seed_dict['test_runs'][test_run_id]['sequences'], \n",
    "            _, _, _, _ \n",
    "        ) = driver.run_testing_from_filepath(\n",
    "            model_cfg_filepath=(configs_dir / 'model.json'),\n",
    "            model_filepath=model_filepath,\n",
    "            data_test_cfg_filepath=(configs_dir / 'data_test.json'),\n",
    "            testing_cfg_filepath=(configs_dir / 'testing.json'),\n",
    "            reproducibility_cfg_filepath=(configs_dir / 'reproducibility.json'),\n",
    "            seed_idx=seed_idx,\n",
    "            log_dir=None,\n",
    "            weights_only=False,\n",
    "            log_outputs_override=True\n",
    "        )\n",
    "\n",
    "    logs.append(seed_dict)\n",
    "\n",
    "logs = pd.DataFrame(logs)\n",
    "\n",
    "# Convert tagged dicts back to dataclasses and recover any items \n",
    "# encoded as FactoryConfig objects.\n",
    "logs = config_utils.serialization.recursive_tagged_dict_to_dataclass(logs)\n",
    "logs = config_utils.serialization.recursive_recover(logs)\n",
    "logs.head()\n",
    "\n",
    "num_test_runs = len(test_run_id_list)\n",
    "print(f\"{num_test_runs} test runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up simpler working dictionary that has just tensors needed.\n",
    "# working_dict = {\n",
    "#     seed_idx : {\n",
    "#         test_run_id : {\n",
    "#             'hidden': get_network_test_output(seed_idx, test_run_id, output_dict='joined', output_key='hidden')[0],\n",
    "#             'hidden_lengths': get_network_test_output(seed_idx, test_run_id, output_dict='joined', output_key='hidden')[1],\n",
    "#             'ground_truth_labels': logs.iloc[seed_idx]['test_runs'][test_run_id]['sequences']['test'].labels,\n",
    "#             'generated_labels': get_network_test_output(seed_idx, test_run_id, output_dict='generated', output_key='labels')\n",
    "#         }\n",
    "#         for test_run_id in test_run_id_list\n",
    "#     }\n",
    "#     for seed_idx in seed_idx_list\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_id_list.sort()\n",
    "print(test_run_id_list)\n",
    "print(type(test_run_id_list))\n",
    "print(test_run_id_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355106ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs.iloc[seed_idx]['test_runs'].keys()\n",
    "# ids = ids.sort()\n",
    "# # for i_string, string in enumerate(ids):\n",
    "# #     if not isinstance(string, str):\n",
    "# #         print(i_string)\n",
    "# #         print(string)\n",
    "# type(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormaps.\n",
    "CMAPS = {}\n",
    "CMAPS[0] = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'lightskyblue_to_darkblue', \n",
    "    [(0, 'lightskyblue'), (1, 'darkblue')]\n",
    ")\n",
    "CMAPS[1] = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'pink_to_maroon', \n",
    "    [(0, 'pink'), (1, 'maroon')]\n",
    ")\n",
    "CMAPS[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdce48a",
   "metadata": {},
   "source": [
    "### Plot each test run in a separate subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a57305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed index.\n",
    "seed_idx = 4\n",
    "\n",
    "# Number of sequences per test run to plot.\n",
    "num_seq_to_plot = 20\n",
    "\n",
    "# Which PCs will be selected for all plots.\n",
    "pcs_to_plot = [0, 1, 2]\n",
    "\n",
    "# Test runs to plot.\n",
    "test_run_id_to_plot_list = test_run_id_list\n",
    "num_test_runs_to_plot = len(test_run_id_to_plot_list)\n",
    "print(test_run_id_to_plot_list)\n",
    "\n",
    "# Prepare figure axes for all test runs (for one seed).\n",
    "# dims = plotting.utils.subplot_dims(num_subplots=len(test_run_id_list) * 3, layout='tall')\n",
    "fig_0, axs_0 = plt.subplots(nrows=4, ncols=num_test_runs_to_plot, subplot_kw={'projection' : '3d'}, squeeze=False, figsize=(8, 3 * 4), constrained_layout=True)\n",
    "\n",
    "for i_test_run, test_run_id in enumerate(test_run_id_to_plot_list):\n",
    "    # Get all joined hidden states and joined lengths.\n",
    "    hidden, hidden_lengths = get_network_test_output(logs, seed_idx, test_run_id, output_dict='joined', output_key='hidden')\n",
    "\n",
    "    # Project all trajectories onto common basis.\n",
    "    common_single_test_run = pca(\n",
    "        unpad_and_concat_sequences(hidden, hidden_lengths),\n",
    "        num_comps=3 if hidden.shape[-1] >= 3 else hidden.shape[-1],\n",
    "        corr=False\n",
    "    )\n",
    "    hidden_projected = hidden @ common_single_test_run['v']\n",
    "    \n",
    "    # Plot hidden state space trajectories and logits.\n",
    "    seq_ind = torch.arange(num_seq_to_plot)\n",
    "    batch_idx = 0 # Should be 0 (indexing into pd.Series object) if entire test set passed as a single batch\n",
    "\n",
    "    for i_seq_idx, seq_idx in enumerate(seq_ind):\n",
    "        # Get current projected sequence.\n",
    "        trajectory = hidden_projected[seq_idx, :hidden_lengths[seq_idx], :]\n",
    "        \n",
    "        # Hacky, but if 2D, just add some zeros for now so that the 3d plotting still works.\n",
    "        if trajectory.shape[1] == 2:\n",
    "            trajectory = torch.cat((trajectory, torch.zeros((trajectory.shape[0], 1))), dim=1)\n",
    "        \n",
    "        # Plot hidden state trajectories.\n",
    "        plotting.plot_trajectory(trajectory, alpha=0.2, ax=axs_0[0, i_test_run], cmap=CMAPS[i_test_run])\n",
    "\n",
    "        axs_0[0, 0].set_xlabel(f'PC {pcs_to_plot[0] + 1}')\n",
    "        axs_0[0, 0].set_ylabel(f'PC {pcs_to_plot[1] + 1}')\n",
    "        axs_0[0, 0].set_zlabel(f'PC {pcs_to_plot[2] + 1}')\n",
    "\n",
    "        # Get BOS, switch, and EOS token positions.\n",
    "        labels_test = logs.iloc[seed_idx]['test_runs'][test_run_id]['sequences']['test'].labels[seq_idx]\n",
    "        ind = {\n",
    "            token : torch.where(labels_test == logs.iloc[seed_idx]['test_runs'][test_run_id]['sequences']['test'].special_tokens[token]['label'])[0]\n",
    "            for token in ['bos', 'switch', 'eos']\n",
    "        }\n",
    "        ind['eos'] -= 1\n",
    "        colors = ['hotpink', 'violet', 'mediumslateblue']\n",
    "\n",
    "        # Plot markers for special tokens.\n",
    "        for i_token, (key, value) in enumerate(ind.items()):\n",
    "            axs_0[0, i_test_run].scatter(\n",
    "                trajectory[value, 0], trajectory[value, 1], trajectory[value, 2], \n",
    "                color=colors[i_token],\n",
    "                marker='o',\n",
    "                facecolors='none',\n",
    "                s=75,\n",
    "                linewidths=2,\n",
    "            )\n",
    "\n",
    "        # Plot eigenspectrum.\n",
    "        axs_0[1, i_test_run].remove()\n",
    "        axs_0[1, i_test_run] = fig_0.add_subplot(4, num_test_runs_to_plot, num_test_runs_to_plot+i_test_run+1)\n",
    "        axs_0[1, i_test_run].stem(\n",
    "            common_single_test_run['eigenspectrum'],\n",
    "            linefmt='forestgreen',\n",
    "            markerfmt='forestgreen',\n",
    "            basefmt='limegreen'\n",
    "        )\n",
    "\n",
    "        # Plot eigenvector components.\n",
    "        axs_0[2, i_test_run].remove()\n",
    "        axs_0[2, i_test_run] = fig_0.add_subplot(4, num_test_runs_to_plot, 2*num_test_runs_to_plot+i_test_run+1)\n",
    "        for i_pc in range(common_single_test_run['v'].shape[1]):    \n",
    "            axs_0[2, i_test_run].plot(\n",
    "                torch.sort(common_single_test_run['v'][:, i_pc])[0],\n",
    "                alpha=0.75,\n",
    "                linewidth=4\n",
    "            )\n",
    "\n",
    "        # Plot loading components.\n",
    "        axs_0[3, i_test_run].remove()\n",
    "        axs_0[3, i_test_run] = fig_0.add_subplot(4, num_test_runs_to_plot, 3*num_test_runs_to_plot+i_test_run+1)\n",
    "        for i_pc in range(common_single_test_run['loadings'].shape[1]):    \n",
    "            axs_0[3, i_test_run].plot(\n",
    "                torch.sort(common_single_test_run['loadings'][:, i_pc])[0],\n",
    "                alpha=0.75,\n",
    "                linewidth=4 \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb08674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model and get recurrent weight matrix.\n",
    "seed_idx = 1\n",
    "test_run_to_analyze_id = test_run_id_list[0]\n",
    "model = logs.iloc[seed_idx]['test_runs'][test_run_to_analyze_id]['model']\n",
    "weight_hh_t = model.rnn.weight_hh_l0.detach().cpu()\n",
    "weight_hh = weight_hh_t.numpy()\n",
    "\n",
    "# Get recurrent weight matrix.\n",
    "fig_2, axs_2 = plt.subplots(2, 1, squeeze=False)\n",
    "axs_2[0, 0].imshow(weight_hh)\n",
    "\n",
    "# Compute stuff.\n",
    "eigenvalues, eigenvectors = np.linalg.eig(weight_hh)\n",
    "spectral_radius = np.sort(np.abs(eigenvalues))[-1]\n",
    "print(spectral_radius)\n",
    "\n",
    "# Plot singular values.\n",
    "out = pca(weight_hh_t)\n",
    "axs_2[1, 0].stem(out['sigmas'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b2ce9",
   "metadata": {},
   "source": [
    "### Plot all test runs on the same subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seed index from above cell. Set number of sequences (per test run) to plot independently here.\n",
    "num_seq_to_plot = 50\n",
    "\n",
    "# PCs to plot.\n",
    "pcs_to_plot = [0, 1, 2]\n",
    "\n",
    "# Prepare single figure axes for all test runs (for one seed).\n",
    "fig_1, axs_1 = plt.subplots(4, 1, subplot_kw={'projection' : '3d'}, figsize=(8, 18), constrained_layout=True)\n",
    "\n",
    "# Find basis for all sequences across all test runs. \n",
    "hidden_all_test_runs, hidden_lengths_all_test_runs = zip(*(\n",
    "    get_network_test_output(logs, seed_idx, test_run_id, output_dict='joined', output_key='hidden') \n",
    "    for test_run_id in test_run_id_list\n",
    "))\n",
    "common_all_test_runs = pca(\n",
    "    unpad_and_concat_iterable_of_sequences(\n",
    "        hidden_all_test_runs, hidden_lengths_all_test_runs\n",
    "    ),\n",
    "    # num_comps=(\n",
    "    #     3 if hidden_all_test_runs[0].shape[-1] >= 3 \n",
    "    #     else hidden_all_test_runs[0].shape[-1]\n",
    "    # ),\n",
    "    num_comps=None,\n",
    "    corr=False\n",
    ")\n",
    "\n",
    "for i_test_run, test_run_id in enumerate(test_run_id_to_plot_list):\n",
    "    # Project trajectories from current test run onto common basis.\n",
    "    hidden_projected = hidden_all_test_runs[i_test_run] @ common_all_test_runs['v']\n",
    "\n",
    "    # Plot hidden state space trajectories and logits.\n",
    "    seq_ind = torch.arange(num_seq_to_plot)\n",
    "\n",
    "    for i_seq_idx, seq_idx in enumerate(seq_ind):\n",
    "        # Get hidden states for current sequence and project onto common basis.\n",
    "        trajectory = hidden_projected[seq_idx, :hidden_lengths_all_test_runs[i_test_run][seq_idx], pcs_to_plot]\n",
    "\n",
    "        # Hacky, but if 2D, just add some zeros for now so that the 3d plotting still works.\n",
    "        if trajectory.shape[1] == 2:\n",
    "            trajectory = torch.cat((trajectory, torch.zeros((trajectory.shape[0], 1))), dim=1)\n",
    "        \n",
    "        # Plot hidden state trajectories.\n",
    "        plotting.plot_trajectory(\n",
    "            trajectory,\n",
    "            alpha=0.2,\n",
    "            ax=axs_1[0],\n",
    "            cmap=CMAPS[i_test_run]\n",
    "        )\n",
    "        axs_1[0].set_xlabel(f'PC {pcs_to_plot[0] + 1}')\n",
    "        axs_1[0].set_ylabel(f'PC {pcs_to_plot[1] + 1}')\n",
    "        axs_1[0].set_zlabel(f'PC {pcs_to_plot[2] + 1}')\n",
    "\n",
    "        # Get BOS, switch, and EOS token positions.\n",
    "        labels_test = logs.iloc[seed_idx]['test_runs'][test_run_id]['sequences']['test'].labels[seq_idx]\n",
    "        ind = {\n",
    "            token : torch.where(labels_test == logs.iloc[seed_idx]['test_runs'][test_run_id]['sequences']['test'].special_tokens[token]['label'])[0]\n",
    "            for token in ['bos', 'switch', 'eos']\n",
    "        }\n",
    "        ind['eos'] -= 1\n",
    "        colors = ['hotpink', 'violet', 'mediumslateblue']\n",
    "\n",
    "        # Plot markers for special tokens.\n",
    "        for i_token, (key, value) in enumerate(ind.items()):\n",
    "            axs_1[0].scatter(\n",
    "                trajectory[value, 0], trajectory[value, 1], trajectory[value, 2], \n",
    "                color=colors[i_token],\n",
    "                marker='o',\n",
    "                facecolors='none',\n",
    "                s=75,\n",
    "                linewidths=2,\n",
    "            )\n",
    "\n",
    "        # Plot eigenspectrum.\n",
    "        axs_1[1].remove()\n",
    "        axs_1[1] = fig_1.add_subplot(4, 1, 2)\n",
    "        axs_1[1].stem(\n",
    "            common_all_test_runs['eigenspectrum'],\n",
    "            linefmt='forestgreen',\n",
    "            markerfmt='forestgreen',\n",
    "            basefmt='limegreen'\n",
    "        )\n",
    "\n",
    "        # Plot eigenvector components.\n",
    "        axs_1[2].remove()\n",
    "        axs_1[2] = fig_1.add_subplot(4, 1, 3)\n",
    "        for i_pc in pcs_to_plot:    \n",
    "            axs_1[2].plot(\n",
    "                torch.sort(common_all_test_runs['v'][:, i_pc])[0],\n",
    "                alpha=0.75,\n",
    "                linewidth=4\n",
    "            )\n",
    "\n",
    "        # Plot loading components.\n",
    "        axs_1[3].remove()\n",
    "        axs_1[3] = fig_1.add_subplot(4, 1, 4)\n",
    "        for i_pc in pcs_to_plot:    \n",
    "            axs_1[3].plot(\n",
    "                torch.sort(common_all_test_runs['loadings'][:, i_pc])[0],\n",
    "                alpha=0.75,\n",
    "                linewidth=4\n",
    "            )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_confusion_matrix(ground_truth_labels, generated_labels, matrix_support=None):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    COUNT_LABEL = 0\n",
    "    EOS_LABEL = 1\n",
    "    \n",
    "    def get_count_from_generated_labels(labels):\n",
    "        ones_ind = torch.nonzero(labels == EOS_LABEL, as_tuple=False)\n",
    "        if ones_ind.numel() < 1:\n",
    "            raise RuntimeError(\"It appears a sequence is lacking an EOS token.\")\n",
    "        eos_idx = ones_ind[0].item()\n",
    "        return torch.sum(labels[:eos_idx] == COUNT_LABEL)\n",
    "\n",
    "    # Get target/generated counts for each sequence.\n",
    "    target_counts = torch.tensor([\n",
    "        torch.sum(labels == COUNT_LABEL) for labels in ground_truth_labels\n",
    "    ])\n",
    "    # generated_counts = torch.tensor([\n",
    "    #         torch.sum(labels == 0) for labels in generated_labels\n",
    "    #     ])\n",
    "    generated_counts = torch.tensor([\n",
    "            get_count_from_generated_labels(labels) for labels in generated_labels\n",
    "        ])\n",
    "    \n",
    "\n",
    "    if matrix_support is None:\n",
    "        # Integers 0 to N, where N is the max target/generated count.\n",
    "        ground_truth_support = torch.unique(target_counts)\n",
    "        generated_support = torch.unique(generated_counts)\n",
    "        matrix_support = torch.arange(\n",
    "            max(ground_truth_support.max(), generated_support.max()) + 1 # Plus one since we need counts\n",
    "        ) \n",
    "\n",
    "    # Construct confusion matrix.\n",
    "    confusion_matrix = torch.full((len(matrix_support), len(matrix_support)), torch.nan)\n",
    "    for i_elem, elem in enumerate(matrix_support):\n",
    "        label_ind = target_counts == elem\n",
    "        confusion_matrix[i_elem, :] = torch.sum(generated_counts[label_ind][:, None] == matrix_support[None, :], dim=0)\n",
    "        \n",
    "    return confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_idx = 4\n",
    "\n",
    "# First get max count across target/generated counts for all test runs to\n",
    "# specify common support. May as well store labels for easier access during\n",
    "# plotting step.\n",
    "labels = {\n",
    "    test_run_id: {\n",
    "        'ground_truth': logs.iloc[seed_idx]['test_runs'][test_run_id]['sequences']['test'].labels,\n",
    "        'generated': get_network_test_output(logs, seed_idx, test_run_id, output_dict='generated', output_key='labels')\n",
    "    }\n",
    "    for test_run_id in test_run_id_to_plot_list\n",
    "}\n",
    "confusion_matrix_support = torch.arange(\n",
    "    max([\n",
    "        max(\n",
    "            max([torch.sum(labels == 0) for labels in labels[test_run_id]['ground_truth']]),\n",
    "            max([torch.sum(labels == 0) for labels in labels[test_run_id]['generated']])\n",
    "        )\n",
    "        for test_run_id in test_run_id_to_plot_list\n",
    "    ])\n",
    "    + 1\n",
    ")\n",
    "\n",
    "# Plot confusion matrices.\n",
    "fig_2, axs_2 = plt.subplots(1, 2, squeeze=False)\n",
    "for i_test_run, test_run_id in enumerate(test_run_id_to_plot_list):\n",
    "    confusion_matrix = construct_confusion_matrix(\n",
    "        ground_truth_labels=labels[test_run_id]['ground_truth'], \n",
    "        generated_labels=labels[test_run_id]['generated'],\n",
    "        matrix_support=confusion_matrix_support\n",
    "    )\n",
    "    i_row, i_col = divmod(i_test_run, num_test_runs_to_plot)\n",
    "    print(confusion_matrix)\n",
    "    print(i_test_run)\n",
    "    axs_2[i_row, i_col].imshow(confusion_matrix)\n",
    "    axs_2[i_row, i_col].set_title(f\"Accuracy = {torch.trace(confusion_matrix/torch.sum(confusion_matrix, dim=None))}\")\n",
    "    print(test_run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# def parse_int_id(test_run_id, pattern=r'(\\d+)$'):\n",
    "#     m = re.search(pattern, test_run_id)\n",
    "#     if not m:\n",
    "#         raise ValueError(f\"Couldn't parse integer id from {test_run_id}\")\n",
    "#     return int(m.group(1))\n",
    "\n",
    "# def coords_from_id(k: int, nA: int, nB: int):\n",
    "#     a_idx = k // nB\n",
    "#     b_idx = k %  nB\n",
    "#     return a_idx, b_idx\n",
    "\n",
    "# def make_grid_index(logs, seed_idx: int, nA: int, nB: int, id_regex=r'(\\d+)$'):\n",
    "#     \"\"\"\n",
    "#     Returns a MultiIndex DataFrame mapping (a_idx, b_idx) -> test_run_id/test_dir\n",
    "#     \"\"\"\n",
    "#     rows = []\n",
    "#     for test_run_id, rec in logs.iloc[seed_idx]['test_runs'].items():\n",
    "#         k = parse_int_id(test_run_id, id_regex)\n",
    "#         a_idx, b_idx = coords_from_id(k, nA, nB)\n",
    "#         rows.append({\n",
    "#             \"a_idx\": a_idx,\n",
    "#             \"b_idx\": b_idx,\n",
    "#             \"test_run_id\": test_run_id,\n",
    "#             \"test_dir\": rec[\"test_dir\"],\n",
    "#         })\n",
    "#     df = pd.DataFrame(rows).set_index([\"a_idx\", \"b_idx\"]).sort_index()\n",
    "#     return df\n",
    "# import pandas as pd\n",
    "\n",
    "# def make_grid_index_by_position(logs, seed_idx: int, nA: int, nB: int):\n",
    "#     \"\"\"\n",
    "#     Build (a_idx, b_idx) -> test_run_id/test_dir using the order the runs\n",
    "#     appear in logs.iloc[seed_idx]['test_runs'] (i.e., directory order).\n",
    "#     Assumes itertools.product with B varying fastest.\n",
    "#     \"\"\"\n",
    "#     # Preserve insertion order (Python 3.7+ dicts do)\n",
    "#     ordered_ids = list(logs.iloc[seed_idx]['test_runs'].keys())\n",
    "\n",
    "#     rows = []\n",
    "#     for k, test_run_id in enumerate(ordered_ids):\n",
    "#         a_idx = k // nB            # A slow axis\n",
    "#         b_idx = k %  nB            # B fast axis\n",
    "#         rec = logs.iloc[seed_idx]['test_runs'][test_run_id]\n",
    "#         rows.append({\n",
    "#             \"a_idx\": a_idx,\n",
    "#             \"b_idx\": b_idx,\n",
    "#             \"test_run_id\": test_run_id,\n",
    "#             \"test_dir\": rec[\"test_dir\"],\n",
    "#         })\n",
    "\n",
    "#     df = pd.DataFrame(rows).set_index([\"a_idx\", \"b_idx\"]).sort_index()\n",
    "\n",
    "#     # Optional sanity checks\n",
    "#     # print(\"grid size:\", len(df), \"expected:\", nA*nB)\n",
    "#     # assert df.index.get_level_values('a_idx').max() <= nA-1\n",
    "#     # assert df.index.get_level_values('b_idx').max() <= nB-1\n",
    "\n",
    "#     return df\n",
    "\n",
    "def make_grid_df(logs, seed_idx: int, num_grid_rows: int, num_grid_cols: int, sort=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Get all test run IDs in dataframe, optionally sort.\n",
    "    test_run_id_list = list(logs.iloc[seed_idx]['test_runs'].keys())\n",
    "    if sort:\n",
    "        test_run_id_list.sort()\n",
    "\n",
    "    # Create new dataframe with multi-level index.\n",
    "    rows = []\n",
    "    for i_test_run, test_run_id in enumerate(test_run_id_list):\n",
    "        i_row = i_test_run // num_grid_rows\n",
    "        i_col = i_test_run % num_grid_rows\n",
    "        rows.append(\n",
    "            {\n",
    "                'row_idx': i_row,\n",
    "                'col_idx': i_col,\n",
    "                'test_run_id': test_run_id,\n",
    "                'test_dir': logs.iloc[seed_idx]['test_runs'][test_run_id]['test_dir']\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    return pd.DataFrame(rows).set_index(['row_idx', 'col_idx'], drop=True, inplace=True).sort_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nA, nB = 18, 6  # your known sizes\n",
    "# # grid_df = make_grid_index(logs, seed_idx=0, nA=nA, nB=nB)\n",
    "# # # Example: get the test_run_id at (a_idx=3, b_idx=9)\n",
    "# # grid_df.loc[(3, 5), \"test_run_id\"]\n",
    "\n",
    "# nA, nB = 18, 6\n",
    "# grid_df = make_grid_index_by_position(logs, seed_idx=0, nA=nA, nB=nB)\n",
    "\n",
    "# # lookup (a_idx=3, b_idx=5)\n",
    "# grid_df.loc[(3, 5), \"test_run_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_network_test_accuracy(logs, seed_idx, test_run_id, level=\"epoch\", metric=\"accuracy\"):\n",
    "#     \"\"\"\n",
    "#     Retrieve accuracy metric from test logs.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     logs : DataFrame of experiment logs\n",
    "#     seed_idx : int, index into logs\n",
    "#     test_run_id : str, which test run\n",
    "#     level : \"epoch\" or \"batch\"\n",
    "#     metric : column name in the log DataFrame\n",
    "#     \"\"\"\n",
    "#     df = logs.iloc[seed_idx]['test_runs'][test_run_id][level]\n",
    "#     if metric not in df.columns:\n",
    "#         raise KeyError(f\"Metric '{metric}' not found in {level} log columns: {df.columns}\")\n",
    "#     return df[metric].iloc[-1]  # last epoch (or batch) entry\n",
    "\n",
    "# def get_network_test_metric(logs, seed_idx: int, test_run_id: str, level='epoch', epoch_idx=None, batch_idx=None, metric='accuracy'):\n",
    "#     df = logs.iloc[seed_idx]['test_runs'][test_run_id][level]\n",
    "    \n",
    "#     if metric not in df.columns: \n",
    "#         raise KeyError(\n",
    "#             f\"metric '{metric}' not in {level} log columns: {df.columns}.\"\n",
    "#         )\n",
    "\n",
    "#     if level == 'epoch':\n",
    "#         if epoch_idx is None:\n",
    "#             return df.iloc[-1]\n",
    "#         else:\n",
    "#             validation_utils.validate_nonneg_int(epoch_idx)\n",
    "#             return df[metric].loc[df['epoch_idx'] == epoch_idx]\n",
    "#     if level == 'batch':\n",
    "#         if epoch_idx is None and batch_idx is None:\n",
    "#             return df.iloc[-1]\n",
    "#         elif not (epoch_idx and batch_idx):\n",
    "#             raise ValueError(\n",
    "#                 f\"If level='batch', epoch_idx and batch_idx must either both be None, or neither may be None.\"\n",
    "#             )\n",
    "#         return df[metric].loc[df['epoch_idx'] == epoch_idx and df['batch_idx'] == 'batch_idx']\n",
    "\n",
    "# A test logger object is returned from testing. Could use its built in retreival functionality to easily access logged values from any given epoch or batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = []\n",
    "# for test_run_id in test_run_id_list:\n",
    "#     acc.append(get_network_test_accuracy(logs, seed_idx=0, test_run_id=test_run_id, metric=\"accuracy\"))\n",
    "#     # print(acc)\n",
    "\n",
    "# accuracy = torch.tensor(acc)\n",
    "\n",
    "# accuracy_grid = torch.full((nA, nB), torch.nan)\n",
    "# for i_A in range(nA):\n",
    "#     for i_B in range(nB):\n",
    "#         accuracy_grid[i_A, i_B] = get_network_test_accuracy(logs, seed_idx=0, test_run_id=grid_df.loc[(i_A, i_B), \"test_run_id\"], metric=\"accuracy\")\n",
    "\n",
    "# accuracy_np = accuracy_grid.numpy()\n",
    "\n",
    "# plt.imshow(accuracy_np)\n",
    "# plt.show()\n",
    "# print(accuracy_np)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "counting_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
